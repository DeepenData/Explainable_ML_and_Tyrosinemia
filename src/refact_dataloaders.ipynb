{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['empirical_mean_for_GRUD', 'test', 'train', 'val'] \n",
      " <KeysViewHDF5 ['X', 'X_hat', 'indicating_mask', 'missing_mask']> \n",
      " <HDF5 dataset \"X\": shape (4, 9, 51), type \"<f4\"> \n",
      " <KeysViewHDF5 ['X']>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "hf = h5py.File('data/datasets.h5', 'r')\n",
    "\n",
    "print(\n",
    "list(hf.keys()),'\\n' , \n",
    "hf['val'].keys(),'\\n',\n",
    "hf['val']['X'],'\\n',\n",
    "hf['train'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DeepenData/.miniconda/envs/torch_aa/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 51]), torch.Size([1, 9, 51]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class LoadDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_len, feature_num):\n",
    "        super(LoadDataset, self).__init__()\n",
    "        self.file_path   = file_path\n",
    "        self.seq_len     = seq_len\n",
    "        self.feature_num = feature_num\n",
    "        #self.model_type  = model_type\n",
    "        \n",
    "        \n",
    "class LoadValTestDataset(LoadDataset):\n",
    "    \"\"\"Loads validation set\"\"\"\n",
    "\n",
    "    def __init__(self, file_path, set_name, seq_len, feature_num):\n",
    "        super(LoadValTestDataset, self).__init__(file_path, seq_len, feature_num)\n",
    "        with h5py.File(self.file_path, 'r') as hf:  # read data from h5 file\n",
    "            self.X = hf[set_name]['X'][:]\n",
    "            self.X_hat = hf[set_name]['X_hat'][:]\n",
    "            self.missing_mask = hf[set_name]['missing_mask'][:]\n",
    "            self.indicating_mask = hf[set_name]['indicating_mask'][:]\n",
    "\n",
    "        # fill missing values with 0\n",
    "        self.X     = np.nan_to_num(self.X)\n",
    "        self.X_hat = np.nan_to_num(self.X_hat)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = (\n",
    "                torch.tensor(idx),\n",
    "                torch.from_numpy(self.X_hat[idx].astype('float32')),\n",
    "                torch.from_numpy(self.missing_mask[idx].astype('float32')),\n",
    "                torch.from_numpy(self.X[idx].astype('float32')),\n",
    "                torch.from_numpy(self.indicating_mask[idx].astype('float32')),\n",
    "            )\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class LoadTrainDataset(LoadDataset):\n",
    "    \"\"\"Loads train set\"\"\"\n",
    "\n",
    "    def __init__(self, file_path, seq_len, feature_num, masked_imputation_task):\n",
    "        super(LoadTrainDataset, self).__init__(file_path, seq_len, feature_num)\n",
    "        self.masked_imputation_task = masked_imputation_task\n",
    "        if masked_imputation_task:\n",
    "            self.artificial_missing_rate = 0.1\n",
    "            assert 0 < self.artificial_missing_rate < 1, 'artificial_missing_rate should be greater than 0 and less than 1'\n",
    "\n",
    "        with h5py.File(self.file_path, 'r') as hf:  # read data from h5 file\n",
    "            self.X = hf['train']['X'][:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        if self.masked_imputation_task:\n",
    "            X = X.reshape(-1)\n",
    "            indices = np.where(~np.isnan(X))[0].tolist()\n",
    "            indices = np.random.choice(indices, round(len(indices) * self.artificial_missing_rate))\n",
    "            X_hat = np.copy(X)\n",
    "            X_hat[indices] = np.nan  # mask values selected by indices\n",
    "            missing_mask = (~np.isnan(X_hat)).astype(np.float32)\n",
    "            indicating_mask = ((~np.isnan(X)) ^ (~np.isnan(X_hat))).astype(np.float32)\n",
    "            X = np.nan_to_num(X)\n",
    "            X_hat = np.nan_to_num(X_hat)\n",
    "            # reshape into time series\n",
    "            X = X.reshape(self.seq_len, self.feature_num)\n",
    "            X_hat = X_hat.reshape(self.seq_len, self.feature_num)\n",
    "            missing_mask = missing_mask.reshape(self.seq_len, self.feature_num)\n",
    "            indicating_mask = indicating_mask.reshape(self.seq_len, self.feature_num)\n",
    "\n",
    "            #if self.model_type in ['Transformer', 'SAITS']:\n",
    "            sample = (\n",
    "                    torch.tensor(idx),\n",
    "                    torch.from_numpy(X_hat.astype('float32')),\n",
    "                    torch.from_numpy(missing_mask.astype('float32')),\n",
    "                    torch.from_numpy(X.astype('float32')),\n",
    "                    torch.from_numpy(indicating_mask.astype('float32')),\n",
    "                )\n",
    "            #else:\n",
    "            #    assert ValueError, f'Error model type: {self.model_type}'\n",
    "        else:\n",
    "            # if training without masked imputation task, then there is no need to artificially mask out observed values\n",
    "            missing_mask = (~np.isnan(X)).astype(np.float32)\n",
    "            X = np.nan_to_num(X)\n",
    "            #if self.model_type in ['Transformer', 'SAITS']:\n",
    "            sample = (\n",
    "                    torch.tensor(idx),\n",
    "                    torch.from_numpy(X.astype('float32')),\n",
    "                    torch.from_numpy(missing_mask.astype('float32')),\n",
    "                )\n",
    "            #else:\n",
    "            #    assert ValueError, f'Error model type: {self.model_type}'\n",
    "        return sample\n",
    "    \n",
    "val    = LoadValTestDataset('data/datasets.h5', 'val', 9, 51)\n",
    "loader_val = DataLoader(val, batch_size=1)\n",
    "train = LoadTrainDataset('data/datasets.h5', seq_len = 9, feature_num = 51, masked_imputation_task= True)\n",
    "loader_train = DataLoader(train, batch_size=1)\n",
    "\n",
    "\n",
    "next(iter(loader_val))[1].shape, next(iter(loader_train))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 51]), torch.Size([1, 9, 51]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UnifiedDataLoader:\n",
    "    def __init__(self, dataset_path, seq_len, feature_num, batch_size, num_workers=4,masked_imputation_task=True):\n",
    "        \"\"\"\n",
    "        dataset_path: path of directory storing h5 dataset;\n",
    "        seq_len: sequence length, i.e. time steps;\n",
    "        feature_num: num of features, i.e. feature dimensionality;\n",
    "        batch_size: size of mini batch;\n",
    "        num_workers: num of subprocesses for data loading;\n",
    "        model_type: model type, determine returned values;\n",
    "        masked_imputation_task: whether to return data for masked imputation task, only for training/validation sets;\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.seq_len = seq_len\n",
    "        self.feature_num = feature_num\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        #self.model_type = model_type\n",
    "        self.masked_imputation_task = masked_imputation_task\n",
    "        #self.train_dataset, self.train_loader = None, None\n",
    "        #self.val_dataset,   self.val_loader   = None, None\n",
    "        #self.test_dataset, self.test_loader, self.test_set_size = None, None, None\n",
    "        \n",
    "    def get_train_val_dataloader(self):\n",
    "        self.train_dataset = LoadTrainDataset(self.dataset_path,        self.seq_len, self.feature_num, self.masked_imputation_task)\n",
    "        self.val_dataset = LoadValTestDataset(self.dataset_path, 'val', self.seq_len, self.feature_num)\n",
    "        #self.train_set_size = self.train_dataset.__len__()\n",
    "        #self.val_set_size = self.val_dataset.__len__()\n",
    "        \n",
    "        \n",
    "        train_sampler = torch.utils.data.RandomSampler(self.train_dataset , replacement=True, num_samples=200)\n",
    "        val_sampler   = torch.utils.data.RandomSampler(self.val_dataset , replacement=True, num_samples=50)\n",
    "        \n",
    "        self.train_loader = DataLoader(self.train_dataset, self.batch_size,  num_workers=self.num_workers, sampler=train_sampler, drop_last=True)\n",
    "        self.val_loader   = DataLoader(self.val_dataset, self.batch_size, num_workers=self.num_workers, sampler=val_sampler, drop_last=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.train_loader, self.val_loader\n",
    "    \n",
    "    \n",
    "ud = UnifiedDataLoader('data/datasets.h5', seq_len = 9, feature_num = 51, batch_size=1)\n",
    "\n",
    "\n",
    "tr, val = ud.get_train_val_dataloader()\n",
    "\n",
    "\n",
    "next(iter(tr))[1].shape, next(iter(val))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAITS_model import SAITS\n",
    "import torch.optim as optim\n",
    "\n",
    "model_args = {'device': 'cuda',\n",
    "            'MIT': True,\n",
    "            'n_groups': 5,\n",
    "            'n_group_inner_layers': 1,\n",
    "            'd_time': 9,\n",
    "            'd_feature': 51,\n",
    "            'dropout': 0.0,\n",
    "            'd_model': 256,\n",
    "            'd_inner': 512,\n",
    "            'n_head': 8,\n",
    "            'd_k': 32,\n",
    "            'd_v': 32,\n",
    "            'input_with_mask': True,\n",
    "            'diagonal_attention_mask': True,\n",
    "            'param_sharing_strategy': 'inner_group'}\n",
    "\n",
    "model = SAITS(**model_args)\n",
    "if  torch.cuda.is_available() and torch.cuda.is_initialized():\n",
    "    model = model.to('cuda')\n",
    "\n",
    "optimizer               = getattr(optim, 'Adam')(model.parameters(), lr= 0.000682774550436755)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "unified_dataloader                = UnifiedDataLoader('data/datasets.h5', seq_len = 9, feature_num = 51, batch_size=2)\n",
    "train_dataloader, val_dataloader  = unified_dataloader.get_train_val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputed_data': tensor([[[ 0.0000e+00,  0.0000e+00,  2.4332e+02,  2.6400e+01,  3.4030e+02,\n",
       "            7.3000e+01,  4.6600e+00,  1.2000e+00,  1.3700e+01,  4.9000e-01,\n",
       "            2.5000e-01,  2.5000e+01,  3.0000e+01,  3.2070e+02,  2.6140e+02,\n",
       "            9.3300e+02,  1.1000e+02, -1.5349e+00,  2.8873e+00, -9.8850e+00,\n",
       "           -8.6182e+00, -5.9854e+00, -1.3297e+01, -9.3981e+00,  1.4324e+01,\n",
       "            3.1930e+01, -2.7893e+01,  1.5711e+01, -3.2754e+01, -3.4161e+00,\n",
       "            3.5652e+00, -2.1237e+01,  7.2518e+00,  2.1809e+00, -2.5737e+01,\n",
       "            1.5099e+01,  1.2411e+00,  8.8086e+00, -2.7516e+01, -2.5688e+01,\n",
       "            8.6853e+00,  1.1588e+01,  4.9058e-02, -2.7654e+01, -1.0823e+01,\n",
       "           -7.1812e+00,  3.0640e+00, -3.1697e+00, -1.4642e+01, -6.6812e+00,\n",
       "            2.2464e+01],\n",
       "          [ 1.0900e+00,  2.6680e+01,  0.0000e+00, -1.2276e+01,  8.6000e+01,\n",
       "            2.0700e+01,  4.1500e+00,  1.0400e+00, -5.3223e+00,  3.1000e-01,\n",
       "            6.0000e-01,  3.3000e+01,  4.6000e+01,  3.5900e+02,  2.4100e+02,\n",
       "            2.6780e+02,  8.1000e+01, -8.8810e+00,  5.5299e+00, -6.8607e+00,\n",
       "            5.0155e+00,  1.1709e+01,  6.8300e+00,  4.6417e+00,  4.0274e+00,\n",
       "            9.6091e+00, -1.6241e+01,  6.6426e+00, -9.0129e+00, -4.7406e+00,\n",
       "           -3.7407e+00, -5.1084e+00,  7.7477e+00, -1.3130e+00, -1.0212e+01,\n",
       "            4.0532e+00,  8.2188e+00,  4.8026e+00, -1.7998e+01, -1.1671e+01,\n",
       "            5.7251e+00,  9.9059e+00, -2.8743e+00, -1.7351e+01, -4.4725e+00,\n",
       "           -9.1025e+00,  2.3368e+00, -7.9950e+00,  1.2974e+00, -1.3985e+01,\n",
       "            2.2366e+00],\n",
       "          [ 9.3000e-01,  3.4590e+01,  0.0000e+00,  1.9685e+00,  2.1390e+02,\n",
       "            5.3800e+01, -7.7099e+00,  1.0500e+00,  1.3700e+01, -3.8180e+00,\n",
       "            4.0000e-02,  3.8000e+01,  6.3000e+01, -1.5481e+00,  3.6800e+02,\n",
       "            2.0140e+02,  8.1000e+01, -4.3899e+00,  3.4702e+00, -1.0798e+01,\n",
       "            3.7843e+00,  8.4793e+00,  3.2634e+00,  3.4340e+00,  5.8608e+00,\n",
       "            8.0623e+00, -1.4624e+01,  7.4590e+00, -7.1680e+00, -5.8778e+00,\n",
       "           -8.8762e+00, -8.5579e-01,  6.8997e+00, -4.0103e-01, -8.6734e+00,\n",
       "            1.3321e+01,  9.7007e-01,  5.1149e+00, -1.7641e+01, -1.4408e+01,\n",
       "            1.2573e+01, -1.5322e+00, -6.2574e+00, -7.1970e+00,  9.7411e-01,\n",
       "           -7.6654e+00, -1.7510e+00, -1.1119e+01,  3.7401e+00, -2.6467e+00,\n",
       "            1.4759e+01],\n",
       "          [ 8.4000e-01,  1.4900e+01, -7.2520e+00,  2.4500e+01,  2.1670e+02,\n",
       "            7.0800e+01,  3.0600e+00,  1.1200e+00,  1.4500e+01, -5.0434e+00,\n",
       "            5.2000e-01,  4.1000e+01,  4.1000e+01,  1.7100e+02,  3.8600e+02,\n",
       "            1.0100e+02,  8.3000e+01, -9.5419e+00,  5.4647e+00, -1.4318e+01,\n",
       "            4.2206e+00,  1.3953e+01,  7.3607e+00,  5.6259e+00,  8.9230e-01,\n",
       "            4.3287e+00, -1.6442e+01,  7.1460e+00, -2.7340e+00, -3.5009e+00,\n",
       "           -1.1783e+01, -1.7095e+00,  8.7078e+00, -2.6641e-01, -8.5551e+00,\n",
       "            1.1328e+01,  3.7569e+00,  3.6145e+00, -2.2157e+01, -1.4301e+01,\n",
       "            1.2725e+01,  4.5497e+00, -5.6950e+00, -1.0380e+01,  3.2104e-01,\n",
       "           -6.5590e+00, -2.0891e+00, -1.2573e+01,  1.1678e+01, -1.0542e+01,\n",
       "            1.1270e+01],\n",
       "          [ 1.0000e+00, -9.0780e+00,  0.0000e+00,  3.2800e+01,  7.0000e+02,\n",
       "            5.7100e+01, -2.8785e+00,  1.0200e+00,  1.3700e+01,  7.7000e-01,\n",
       "            4.0000e-02,  4.8000e+01,  5.1000e+01,  1.2800e+02,  1.8381e+01,\n",
       "            3.9600e+01,  8.4000e+01, -3.6065e+00,  3.8270e+01,  7.6800e+01,\n",
       "            7.5840e+01,  3.8870e+01, -1.3571e+01,  2.2648e+02,  2.2450e+01,\n",
       "            1.0700e+00,  5.0000e-02,  6.0000e-02,  7.0000e-02,  2.1000e-01,\n",
       "            3.0000e-02,  2.1000e-01,  1.0000e-02,  1.0200e+00,  2.2100e+01,\n",
       "            2.1770e+01,  2.2336e+02,  2.1390e+01,  4.4400e+01,  0.0000e+00,\n",
       "            1.0318e+02,  1.4080e+01,  1.5000e-01,  1.0000e-02,  1.0000e-02,\n",
       "            8.0000e-02,  9.8044e+00,  7.0000e-02,  2.5000e-01,  2.5000e-01,\n",
       "            3.0000e-02],\n",
       "          [ 8.6000e-01, -6.7515e+00,  0.0000e+00,  1.7310e+01,  1.4313e+02,\n",
       "            3.5200e+00,  4.0662e+01, -1.0465e+01, -1.9052e+01, -5.1293e+00,\n",
       "           -1.3823e+01,  3.2000e+01,  9.2289e+00,  1.1800e+02,  4.4900e+02,\n",
       "            2.8100e+01, -7.5848e+00,  1.7021e+02,  4.9740e+01,  7.7940e+01,\n",
       "            1.3549e+02,  5.5060e+01,  1.0953e+02,  1.4313e+02,  1.9770e+01,\n",
       "            1.1900e+00,  1.4000e-01,  3.0000e-02,  5.0000e-02,  3.0000e-02,\n",
       "            4.0000e-02,  3.7921e+00,  1.0000e-02,  7.4000e-01,  2.6730e+01,\n",
       "            1.2589e+00,  2.1844e+02,  1.7310e+01, -2.7993e+01,  5.0000e-02,\n",
       "            1.7592e+02,  1.1090e+01,  1.5000e-01, -3.9595e+00,  2.0000e-02,\n",
       "            2.0000e-02,  1.0000e-02,  4.0000e-02,  5.5439e+00,  3.9000e-01,\n",
       "            3.0000e-02],\n",
       "          [ 7.7000e-01,  1.6780e+01,  0.0000e+00,  2.2560e+01,  2.1566e+02,\n",
       "            9.7000e+00,  2.2233e+01, -5.0450e+00, -9.7148e+00,  4.0121e+00,\n",
       "           -7.2878e+00, -5.7080e+00,  2.2487e-01, -5.9088e+00,  6.1378e+00,\n",
       "            1.0516e+01,  7.5167e+00,  1.8659e+02,  2.1480e+01,  5.6180e+01,\n",
       "            5.1728e+00,  5.0010e+01,  1.1730e+02,  2.1566e+02,  2.4260e+01,\n",
       "            2.2017e+00,  1.2000e-01,  1.3000e-01,  5.0000e-02,  6.0000e-02,\n",
       "            3.0000e-02,  1.9000e-01,  1.0000e-02,  8.7000e-01,  3.8430e+01,\n",
       "            3.1050e+01,  2.1280e+02,  2.2560e+01,  9.6800e+00,  0.0000e+00,\n",
       "            1.8981e+02,  1.4890e+01,  1.4000e-01,  4.0000e-02,  1.0000e-02,\n",
       "            4.0000e-02,  4.0000e-02,  4.0000e-02,  3.8000e-01,  2.7000e-01,\n",
       "            1.0000e-02],\n",
       "          [ 7.4000e-01,  2.3300e+01,  0.0000e+00,  1.7680e+01,  2.8718e+02,\n",
       "           -6.3445e+00,  1.1067e+01, -3.9258e+00, -1.4078e+01,  4.6532e+00,\n",
       "           -1.0838e+01, -8.8459e+00, -2.8545e+00, -7.2527e+00,  9.7345e+00,\n",
       "            1.9844e+01,  6.3287e+00,  1.2410e+02,  4.9490e+01,  9.1940e+01,\n",
       "            1.0881e+02,  5.6590e+01,  6.5400e+01,  2.8718e+02,  2.8710e+01,\n",
       "            1.6000e+00,  1.0000e-01,  5.0000e-02,  5.0000e-02,  2.3000e-01,\n",
       "            6.0000e-02,  2.2000e-01,  1.0000e-02,  1.1700e+00,  2.3280e+01,\n",
       "            2.7610e+01,  2.4928e+02,  1.7680e+01,  2.5950e+01,  2.4000e-01,\n",
       "            1.3768e+02,  2.0042e+00,  1.8000e-01,  5.0000e-02,  3.0000e-02,\n",
       "            9.0000e-02,  5.0000e-02,  7.0000e-02,  5.3000e-01,  3.6000e-01,\n",
       "            2.0000e-02],\n",
       "          [-6.8432e+00, -5.6720e+00, -6.5588e+00, -4.4213e+00, -1.0226e+01,\n",
       "           -2.2688e+00,  2.8010e+00,  4.4524e+00, -5.0213e+00,  4.5087e+00,\n",
       "           -6.9155e+00, -5.4230e+00, -1.0358e+01,  1.4246e+00,  1.8785e+00,\n",
       "            6.7206e+00,  5.5420e+00,  1.6140e+02,  3.2680e+01,  4.0934e+00,\n",
       "            1.0960e+02,  5.3550e+01,  7.3830e+01,  1.1802e+02,  2.4510e+01,\n",
       "            9.2000e-01,  9.0000e-02,  1.0000e-02,  1.0180e+01,  1.3000e-01,\n",
       "            4.0000e-02,  3.3129e+00,  1.0000e-02,  7.4000e-01,  6.0984e+00,\n",
       "            2.5490e+01,  2.1989e+02,  1.4220e+01,  9.9800e+00,  0.0000e+00,\n",
       "            1.3351e+02,  1.1130e+01,  1.8246e+00,  1.0000e-02,  4.0000e-02,\n",
       "            5.0000e-02,  3.0000e-02,  4.0000e-02,  3.6000e-01,  2.6000e-01,\n",
       "            2.0000e-02]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  2.4332e+02,  2.6400e+01,  3.4030e+02,\n",
       "            7.3000e+01,  4.6600e+00,  1.2000e+00,  1.3700e+01,  4.9000e-01,\n",
       "            2.5000e-01,  2.5000e+01,  3.0000e+01,  3.2070e+02,  1.7640e+01,\n",
       "            9.3300e+02,  1.1000e+02,  2.6725e+00,  2.0985e+00, -2.4184e+00,\n",
       "           -1.4248e+01, -9.5418e+00, -1.4394e+01, -1.3020e+01,  1.4998e+01,\n",
       "            3.3104e+01, -1.9913e+01,  1.3440e+01, -3.1026e+01,  4.6772e+00,\n",
       "            8.2406e+00, -2.5961e+01,  7.8254e-01,  2.7660e+00, -2.3900e+01,\n",
       "            8.3452e+00, -6.0874e+00,  9.9055e+00, -2.1288e+01, -1.8753e+01,\n",
       "            6.4359e+00,  1.0821e+01,  6.2571e+00, -2.9031e+01, -1.2464e+01,\n",
       "           -7.4628e+00,  7.0551e+00,  1.5362e+00, -1.9432e+01, -5.3565e+00,\n",
       "            1.9370e+01],\n",
       "          [ 1.0900e+00,  2.6680e+01,  0.0000e+00,  2.4900e+01,  8.6000e+01,\n",
       "            2.0700e+01,  4.1500e+00,  1.0400e+00,  1.3500e+01,  3.1000e-01,\n",
       "            6.0000e-01,  3.3000e+01,  4.6000e+01,  3.5900e+02,  2.4100e+02,\n",
       "            2.6780e+02,  8.1000e+01, -8.7293e+00,  4.8688e+00, -6.6617e+00,\n",
       "            5.3208e+00,  1.1463e+01,  6.5756e+00,  5.4670e+00,  4.2371e+00,\n",
       "            9.2976e+00, -1.6745e+01,  6.5054e+00, -8.6774e+00, -4.8268e+00,\n",
       "           -3.4814e+00, -6.3962e+00,  8.2662e+00, -2.1293e+00, -1.0375e+01,\n",
       "            3.8952e+00,  8.2510e+00,  4.1112e+00, -1.9552e+01, -1.1879e+01,\n",
       "            5.8222e+00,  9.2927e+00, -3.0706e+00, -1.7570e+01, -3.8675e+00,\n",
       "           -7.7678e+00,  1.5266e+00, -6.6188e+00,  1.6815e+00, -1.4711e+01,\n",
       "            3.1115e+00],\n",
       "          [ 6.1678e+00,  3.4590e+01,  0.0000e+00,  2.1300e+01,  2.1390e+02,\n",
       "            5.3800e+01,  3.9800e+00,  1.0500e+00,  1.3700e+01,  5.1000e-01,\n",
       "            4.0000e-02,  3.8000e+01,  6.3000e+01,  2.2000e+02,  9.3545e+00,\n",
       "            1.3033e+01,  8.1000e+01, -4.8313e+00,  8.4415e+00, -5.5365e+00,\n",
       "           -4.9968e+00,  1.0016e+01,  5.2477e+00,  3.7555e+00, -3.4211e+00,\n",
       "            5.9216e-01, -1.5068e+00,  4.2280e+00,  4.2798e+00,  9.6900e+00,\n",
       "           -7.9135e+00, -4.5655e+00,  2.5833e+00, -5.8259e-01, -9.6161e-01,\n",
       "           -7.3768e-01, -4.8648e+00,  4.7361e+00, -1.2039e+01, -2.1440e+00,\n",
       "            4.4477e+00,  7.3299e+00,  2.1612e+00, -1.0642e+01, -9.0184e-01,\n",
       "           -6.6000e+00,  4.6036e+00, -5.4043e+00,  8.6050e+00, -6.4253e+00,\n",
       "            2.1635e+00],\n",
       "          [ 8.4000e-01,  1.4900e+01, -7.4353e+00,  2.4500e+01,  2.1670e+02,\n",
       "            7.0800e+01,  3.0600e+00, -1.9573e+01,  1.4500e+01,  1.7000e-01,\n",
       "            5.2000e-01,  4.1000e+01,  4.1000e+01,  1.7100e+02,  3.8600e+02,\n",
       "            1.0100e+02,  8.3000e+01, -9.7930e+00,  4.7047e+00, -1.4643e+01,\n",
       "            4.0731e+00,  1.4105e+01,  7.8967e+00,  5.3357e+00,  9.2504e-01,\n",
       "            3.5624e+00, -1.7052e+01,  6.7760e+00, -3.2504e+00, -2.8958e+00,\n",
       "           -1.1898e+01, -1.2717e+00,  8.6869e+00,  4.4327e-01, -9.5995e+00,\n",
       "            1.1008e+01,  4.0080e+00,  3.6631e+00, -2.1094e+01, -1.4606e+01,\n",
       "            1.2698e+01,  3.7209e+00, -5.3521e+00, -1.0194e+01,  2.1386e-01,\n",
       "           -4.9931e+00, -2.1062e+00, -1.2102e+01,  1.1557e+01, -1.0154e+01,\n",
       "            1.1370e+01],\n",
       "          [ 1.0000e+00,  2.7600e+01,  0.0000e+00,  3.2800e+01,  7.0000e+02,\n",
       "            5.7100e+01,  1.2260e+01,  1.0200e+00,  1.3700e+01,  7.7000e-01,\n",
       "            4.0000e-02,  4.8000e+01,  5.1000e+01,  1.2800e+02,  1.8510e+01,\n",
       "            4.2099e+01,  8.4000e+01, -5.0658e+00,  3.8270e+01,  7.6800e+01,\n",
       "            7.5840e+01,  3.8870e+01,  5.9260e+01,  2.2648e+02,  2.2450e+01,\n",
       "            1.0700e+00,  5.0000e-02,  6.0000e-02,  7.0000e-02,  2.1000e-01,\n",
       "            3.0000e-02,  2.1000e-01,  1.0000e-02,  1.0200e+00,  2.2100e+01,\n",
       "            2.1770e+01,  2.2336e+02,  2.1390e+01,  4.4400e+01,  0.0000e+00,\n",
       "            1.0318e+02,  1.4080e+01,  1.5000e-01,  1.0000e-02,  1.0000e-02,\n",
       "            8.0000e-02,  4.0000e-02,  7.0000e-02,  1.7185e+01,  2.5000e-01,\n",
       "            3.0000e-02],\n",
       "          [ 8.6000e-01, -2.6035e+00,  0.0000e+00,  1.7310e+01, -2.1948e+01,\n",
       "            3.5200e+00,  4.0662e+01, -1.4271e+01, -1.3748e+01, -1.9293e+00,\n",
       "           -6.7409e+00,  3.2000e+01,  3.5000e+01,  1.1800e+02,  4.4900e+02,\n",
       "            2.5205e+00, -9.0626e+00,  1.7021e+02,  4.9740e+01,  7.7940e+01,\n",
       "            1.8940e+01,  5.5060e+01,  1.0953e+02,  1.4313e+02,  1.9770e+01,\n",
       "            1.1900e+00, -1.2232e+01,  3.0000e-02,  5.0000e-02,  3.0000e-02,\n",
       "            4.0000e-02,  2.1000e-01,  1.0000e-02,  7.4000e-01,  2.6730e+01,\n",
       "            2.1010e+01,  2.1844e+02,  1.4899e+00,  3.5200e+00,  5.0000e-02,\n",
       "            1.7592e+02,  1.1090e+01, -8.5100e+00,  5.0000e-02,  2.0000e-02,\n",
       "            2.0000e-02,  8.1955e+00,  4.0000e-02,  3.7000e-01,  3.9000e-01,\n",
       "            3.0000e-02],\n",
       "          [ 7.7000e-01,  1.6780e+01,  0.0000e+00,  2.2560e+01,  2.1566e+02,\n",
       "            9.7000e+00,  2.2233e+01, -1.0643e+00, -1.1027e+01,  2.6952e+00,\n",
       "           -1.3718e+01, -6.7021e+00, -3.3901e+00, -5.0084e+00,  6.9540e+00,\n",
       "            1.6328e+01,  8.5621e+00,  1.8659e+02,  2.1480e+01,  5.6180e+01,\n",
       "            1.0316e+02,  5.0010e+01,  1.1730e+02,  2.1566e+02,  2.4260e+01,\n",
       "            1.0700e+00,  1.2000e-01,  1.3000e-01,  5.0000e-02,  6.0000e-02,\n",
       "            3.0000e-02, -1.6177e+00,  1.0000e-02,  8.7000e-01,  3.8430e+01,\n",
       "            3.1050e+01,  2.1280e+02,  2.2560e+01,  9.6800e+00,  0.0000e+00,\n",
       "            1.8981e+02,  1.4890e+01,  1.4000e-01,  4.0000e-02,  1.0000e-02,\n",
       "            4.0000e-02,  4.0000e-02, -2.7500e+00,  3.8000e-01,  2.7000e-01,\n",
       "            1.0000e-02],\n",
       "          [ 7.4000e-01,  2.3300e+01,  0.0000e+00,  1.7680e+01,  2.8718e+02,\n",
       "           -7.4719e+00,  1.1067e+01, -5.5557e+00, -1.4063e+01,  4.0221e+00,\n",
       "           -1.2259e+01, -1.2313e+01, -3.4058e+00, -7.8558e+00,  9.6399e+00,\n",
       "            1.8510e+01,  6.0104e+00, -3.0205e+00,  4.9490e+01,  9.1940e+01,\n",
       "            1.0881e+02,  5.6590e+01,  6.5400e+01,  2.8718e+02,  2.8710e+01,\n",
       "            1.6000e+00,  1.0000e-01,  5.0000e-02,  5.0000e-02,  2.3000e-01,\n",
       "           -1.1836e+01,  2.2000e-01,  1.0000e-02,  1.1700e+00,  2.3280e+01,\n",
       "            2.7610e+01,  2.4928e+02,  1.7680e+01,  2.5950e+01,  2.4000e-01,\n",
       "            1.3768e+02,  2.2276e+00,  1.8000e-01,  5.0000e-02, -1.0402e+01,\n",
       "            9.0000e-02,  5.0000e-02,  7.0000e-02,  5.3000e-01,  3.6000e-01,\n",
       "            2.0000e-02],\n",
       "          [ 2.0496e+00, -1.0143e+01,  1.9575e+00,  1.9660e+00, -2.5026e+00,\n",
       "           -8.0276e-01,  2.6250e-01,  6.5017e+00, -1.0404e+01, -1.8090e+00,\n",
       "           -1.0693e+01, -2.0176e+00, -3.6554e+00,  1.5534e+00,  5.1391e+00,\n",
       "            2.6410e+00,  7.9388e+00,  1.6140e+02,  3.2680e+01,  7.3690e+01,\n",
       "            1.0960e+02,  5.3550e+01,  7.3830e+01,  6.0516e+00,  2.4510e+01,\n",
       "           -1.2270e+00,  9.0000e-02,  1.0000e-02,  5.0000e-02,  1.3000e-01,\n",
       "            4.0000e-02,  1.6000e-01,  1.0000e-02,  7.4000e-01,  2.5740e+01,\n",
       "            2.5490e+01,  4.6039e+00,  1.4220e+01,  9.9800e+00,  0.0000e+00,\n",
       "            1.3351e+02,  1.1130e+01,  1.5000e-01,  1.0000e-02,  4.0000e-02,\n",
       "           -1.7018e+00,  3.0000e-02,  4.0000e-02,  3.6000e-01,  2.6000e-01,\n",
       "            2.0000e-02]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'reconstruction_loss': tensor(59.8644, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'imputation_loss': tensor(53.9966, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'reconstruction_MAE': tensor(58.9759, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'imputation_MAE': tensor(53.9966, device='cuda:0', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total_loss_val = 0\n",
    "for data in train_dataloader:\n",
    "    model.train()\n",
    "    model.to('cuda')\n",
    "    indices, X, missing_mask, X_holdout, indicating_mask = map(lambda x: x.to('cuda'), data)\n",
    "    inputs = {'indices': indices, 'X': X, 'missing_mask': missing_mask,'X_holdout': X_holdout, 'indicating_mask': indicating_mask}\n",
    "    results = model(inputs, 'train')\n",
    "    #results = result_processing(results, args)\n",
    "    #optimizer.zero_grad()\n",
    "    #results['total_loss'].backward()\n",
    "    #optimizer.step()\n",
    "    \n",
    "    \n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "317c545ac1557983df223dc9dc6da11914262073b7c77422002e9cb73db54a4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
